import whisperx
import torch
import gradio as gr
import os
from datetime import datetime

# ğŸ” Cargar token desde variable de entorno
HF_TOKEN = os.getenv("HF_TOKEN")

# ğŸ–¥ï¸ Detectar si hay GPU
device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸ™ï¸ FunciÃ³n principal para procesar audio
def transcribir(audio_path):
    # Cargar modelo de transcripciÃ³n
    model = whisperx.load_model("medium", device=device, compute_type="float32", language="es")
    transcription = model.transcribe(audio_path)

    # Alinear palabras
    model_a, metadata = whisperx.load_align_model(language_code="es", device=device)
    aligned = whisperx.align(transcription["segments"], model_a, metadata, audio_path, device)

    # DiarizaciÃ³n
    diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN)
    diarize_segments = diarize_model(audio_path)

    # FusiÃ³n manual
    from copy import deepcopy
    segments = deepcopy(aligned["segments"])
    for segment in segments:
        for diar in diarize_segments["segments"]:
            if diar["start"] <= segment["start"] < diar["end"]:
                segment["speaker"] = diar["speaker"]
                break

    # Formar texto
    full_text = "\n".join([f"[{seg.get('speaker', 'SPEAKER_X')}] {seg['text']}" for seg in segments])
    timestamp = datetime.now().strftime("GrabaciÃ³n %Y-%m-%d a las %H.%M.%S")
    return f"ğŸ•’ {timestamp}\n\n" + full_text

# ğŸ›ï¸ Interfaz de usuario
ui = gr.Interface(
    fn=transcribir,
    inputs=gr.Audio(type="filepath", label="Sube tu audio"),
    outputs=gr.Textbox(label="TranscripciÃ³n con DiarizaciÃ³n"),
    title="ğŸ§  Transcriptor WhisperX + DiarizaciÃ³n",
    description="Sube un audio y obtÃ©n la transcripciÃ³n con detecciÃ³n de voces."
)

# ğŸš€ Lanzar app
if __name__ == "__main__":
    ui.launch()